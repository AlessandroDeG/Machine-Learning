{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1kKJk_rJC34"
      },
      "source": [
        "# Machine Learning 2021/2022\n",
        "## Assignment 2: Probabilistic Modeling, Unsupervised Learning, Reinforcement Learning\n",
        "Deadline: 20th of December 2021 9pm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLtJ0vlKJReh"
      },
      "source": [
        "First name: Alessandro  \n",
        "Last name: De Grandi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8xajf5MJ8Ud"
      },
      "source": [
        "## About this assignment\n",
        "\n",
        "In this assignment you will further deepen your understanding of Probabilistic Modeling, Unsupervised Learning, and Reinforcement Learning (RL).\n",
        "\n",
        "## Submission instructions\n",
        "\n",
        "Please write your answers, equations, and code directly in this python notebook and print the final result to pdf (File > Print).\n",
        "Make sure that code has appropriate line breaks such that all code is visible in the final pdf.\n",
        "If necessary, select A3 for the PDF size to prevent content from being clipped.\n",
        "\n",
        "The final pdf must be named name.lastname.pdf and uploaded to the iCorsi website before the deadline expires. Late submissions will result in 0 points.\n",
        "\n",
        "**Also share this notebook (top right corner 'Share') with teaching.idsia@gmail.com during submission.**\n",
        "\n",
        "**Keep your answers brief and respect the sentence limits in each question (answers exceeding the limit are not taken into account)**.\n",
        "\n",
        "Note that there are a total of **140 points in this assignment**.\n",
        "\n",
        "Learn more about python notebooks and formatting here: https://colab.research.google.com/notebooks/welcome.ipynb\n",
        "\n",
        "## How to get help\n",
        "\n",
        "We encourage you to use the tutorials to ask questions or to discuss exercises with other students.\n",
        "However, do not look at any report written by others or share your report with others.\n",
        "Violation of that rule will result in 0 points for all students involved. For further questions you can contact the respective TA:\n",
        "\n",
        "- Probabilistic Modeling: louis@idsia.ch\n",
        "- Unsupervised Learning: anand@idsia.ch\n",
        "- Reinforcement Learning: dylan.ashley@idsia.ch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpyRFlxOqRJH"
      },
      "source": [
        "## 1 Probabilistic Modeling (20p)\n",
        "\n",
        "If $X \\sim \\text{Rayleigh}(\\lambda)$ is a Rayleigh random variable then its probability density function is given by\n",
        "\\begin{equation*}\n",
        "    p(x; \\lambda) = \\frac{2x}{\\lambda} e^{-\\frac{x^2}{\\lambda}} \\ \\ \\text{for} \\ \\ x \\geq 0.\n",
        "\\end{equation*}\n",
        "Let $(X_1, X_2, \\ldots, X_N)$ be a random sample from the Rayleigh distribution with parameter $\\lambda$ and let $\\mathcal{D} = (x_1, x_2, \\ldots, x_N)$ be its realization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubdJF8bbqRJI"
      },
      "source": [
        "### Question 1.1 (4p)\n",
        "\n",
        "Derive the likelihood function of $\\lambda$ given $\\mathcal{D}$.  Include all intermediate steps. Simplify sums and products.\n",
        "\n",
        "$$\\mathcal{L}(\\lambda; \\mathcal{D}) = ?$$\n",
        "\n",
        "---\n",
        "\n",
        "**ANSWER HERE**\n",
        "$$\\mathcal{L}(\\lambda; \\mathcal{D}) =\\prod_{i=1}^{N} p(x_{i}; \\lambda) = \\prod_{i=1}^{N}\\frac{2x_{i}}{\\lambda} e^{-\\frac{x_{i}^2}{\\lambda}} = \\frac{2^N}{λ^N}e^{\\sum_{i=1}^N\\frac{-x_i^2}{λ}}\\prod_{i=1}^{N}x_i$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "id9Gzfn-qRJI"
      },
      "source": [
        "### Question 1.2 (2p)\n",
        "\n",
        "Derive the log-likelihood. Include all intermediate steps.\n",
        "\n",
        "$$\\ln \\mathcal{L}(\\lambda; \\mathcal{D}) = ?$$\n",
        "\n",
        "---\n",
        "\n",
        "**ANSWER HERE**\n",
        "\n",
        "$\\ln \\mathcal{L}(\\lambda; \\mathcal{D}) = ln(\\frac{2^N}{λ^N}) + ln(e^{\\sum_{i=1}^N\\frac{-x_i^2}{λ}}) + ln(\\prod_{i=1}^{N}x_i) =$\n",
        "$= Nln(2)-Nln(\\lambda) + {\\sum_{i=1}^N\\frac{-x_i^2}{λ}}+ \\sum_{i=1}^{N}ln(x_i)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XcEbEDS4qRJJ"
      },
      "source": [
        "### Question 1.3 (6p)\n",
        "\n",
        "Derive the maximum likelihood estimate for $\\lambda$. Include all intermediate steps.\n",
        "\n",
        "---\n",
        "\n",
        "**ANSWER HERE**\n",
        "$$ \\frac{\\partial ln \\mathcal{L}(\\lambda; \\mathcal{D})}{\\partialλ}=0 $$  \n",
        "$$\\frac{-N}{λ}+ \\frac{1}{λ^2}\\sum_{i=1}^{N}x_{i}^2 =0 $$\n",
        "$$  λ = \\frac{\\sum_{i=1}^{N}x_{i}^2}{N} $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUnIB-RgqRJJ"
      },
      "source": [
        "### Question 1.4 (8p)\n",
        "\n",
        "Now, suppose we have an inverse gamma prior with parameters $\\alpha$ and $\\beta$ on $\\lambda$, i.e. the probability density function of the prior is\n",
        "\\begin{equation*}\n",
        "    p(\\lambda; \\alpha, \\beta) = \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} \\lambda^{-(\\alpha + 1)} e^{-\\beta / \\lambda}\n",
        "\\end{equation*}\n",
        "where $\\Gamma(\\cdot)$ is the gamma function defined as $\\Gamma(t) = \\int_0^\\infty u^{t-1} e^{-u} \\mathrm{d}u$.\n",
        "\n",
        "What is the maximum a posteriori estimate for $\\lambda$? Include all intermediate steps.\n",
        "\n",
        "---\n",
        "\n",
        "**ANSWER HERE**\n",
        "\n",
        "\n",
        "$p(\\lambda|D) = \\frac{p(D|λ)p(\\lambda)}{p(D)}$\n",
        "\n",
        "$ \\lambda_{MAP}= argmax(p(\\lambda|D))$\n",
        "\n",
        "$ln(p(\\lambda|D)=ln(\\frac{p(D|λ)p(\\lambda)}{p(D)})=ln(p(D|λ))+ln(p(\\lambda))-ln(p(D))$\n",
        "\n",
        "$ \\frac{\\partial ln(p(\\lambda|D))}{\\partialλ}=\\frac{\\partial ln(p(D|lambda))}{\\partialλ}+\\frac{\\partial ln(p(\\lambda))}{\\partialλ}=0$\n",
        "\n",
        "$\\frac{\\partial ln(p(D|lambda))}{\\partialλ}=\\frac{-N}{λ}+ \\frac{1}{λ^2}\\sum_{i=1}^{N}x_{i}^2 $\n",
        "\n",
        "$\\frac{\\partial ln(p(\\lambda))}{\\partialλ}=\\frac{\\partial}{\\partialλ}ln(\\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} \\lambda^{-(\\alpha + 1)} e^{-\\beta / \\lambda})= -\\frac{\\alpha+1}{\\lambda}+\\frac{\\beta}{\\lambda^2}$\n",
        "\n",
        "$\\frac{-N}{λ}+ \\frac{1}{λ^2}\\sum_{i=1}^{N}x_{i}^2 -\\frac{\\alpha+1}{\\lambda}+\\frac{\\beta}{\\lambda^2}=0$\n",
        "\n",
        "$-N+ \\frac{1}{λ}\\sum_{i=1}^{N}x_{i}^2 -(\\alpha+1)+\\frac{\\beta}{\\lambda}=0$\n",
        "\n",
        "$\\lambda = \\frac{(\\sum_{i=1}^{N}x_{i}^2) + \\beta}{-N+\\alpha+1} $\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OjZttfAR2Vo"
      },
      "source": [
        "## 2 Unsupervised Learning (30p)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyWHcP4WCRuO"
      },
      "source": [
        "### Question 2.1 (5p)\n",
        "Given a dataset of points in 1-dimension i.e. $X = \\{0, 2, 4, 8, 12 \\}$, initialize the k-Means algorithm with 2 cluster center(s) as $\\mu_1$ = 0 and $\\mu_2$ = 12. Perform 1 iteration (1 E-step and 1 M-step) of the k-Means algorithm with the given cluster initializations.  Compute cluster assignments for all datapoints and cluster centers at the end of the iteration. (Show **all** intermediate computation steps).\n",
        "\n",
        "---\n",
        "\n",
        "**ANSWER HERE**\n",
        "\n",
        "$X = \\{x_0, x_1, x_2, x_3, x_4 \\}$\n",
        "\n",
        "$C_1$ center is $\\mu_1$\n",
        "\n",
        "$C_2$ center is $\\mu_2$\n",
        "\n",
        "Compute Euclidian distance between points and cluster centers:\n",
        "\n",
        "$d(x_0,\\mu_1)=0 , d(x_0,\\mu_2)=12$\n",
        "\n",
        "$d(x_1,\\mu_1)=2 , d(x_1,\\mu_2)=10$\n",
        "\n",
        "$d(x_2,\\mu_1)=4 , d(x_2,\\mu_2)=8$\n",
        "\n",
        "$d(x_3,\\mu_1)=8 , d(x_3,\\mu_2)=4$\n",
        "\n",
        "$d(x_4,\\mu_1)=12 , d(x_4,\\mu_2)=0$\n",
        "\n",
        "Cluster assignments, assign point to nearest cluster:\n",
        "\n",
        "$C_1=(x_0,x_1,x_2)$\n",
        "\n",
        "$C_2=(x_3,x_4)$\n",
        "\n",
        "Cluster centers:\n",
        "\n",
        "$\\mu_1= \\frac{x_0+x_1+x_2}{3}=2$\n",
        "\n",
        "$\\mu_2=\\frac{x_3+x_4}{2}=10$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJiP3KxzGBn5"
      },
      "source": [
        "### Question 2.2 (10p)\n",
        "For the k-Means algorithm, given a dataset $X = \\{ x_1, x_2, ... , x_i, ... , x_n \\}$ of $n$ points where $x_i \\in \\mathbb{R}^d$. We want to cluster this into $k$ clusters with centers $\\mu = (\\mu_1, ..., \\mu_k)$. Show that k-Means is guaranteed to converge to a local optimum. Convergence criteria is said to have been met in k-Means when no change of cluster assignment happens for any datapoint between iteration $t$ and $t+1$. *Hint:* Prove that the loss function decreases monotonically in each iteration until convergence (prove it separately for both the E-step and M-step).\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**ANSWER HERE**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6uOgFnjIgI8"
      },
      "source": [
        "### Question 2.3 (5p)\n",
        "Find the Eigendecomposition (i.e. eigenvalues and eigenvectors) for \\\\\n",
        "\\begin{equation}\n",
        " A = \\begin{bmatrix}\n",
        " 3 & 5 \\\\ 4 & 2\n",
        " \\end{bmatrix}\n",
        " \\end{equation}\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**ANSWER HERE**\n",
        "\\begin{equation}\n",
        " I = \\begin{bmatrix}\n",
        " 1 & 0 \\\\ 0 & 1\n",
        " \\end{bmatrix}\n",
        " \\end{equation}\n",
        " \\begin{equation}\n",
        " EigenValues: det(A-λI)=0\n",
        " \\end{equation}\n",
        "  \\begin{equation}\n",
        " A-λI =  \\begin{bmatrix}  3 & 5 \\\\ 4 & 2 \\end{bmatrix} - \\begin{bmatrix}  λ & 0 \\\\ 0 & λ \\end{bmatrix} = \\begin{bmatrix}  3-λ & 5 \\\\ 4 & 2-λ \\end{bmatrix}\n",
        "  \\end{equation}\n",
        "   \\begin{equation}\n",
        " det(A-λI)= (3-λ)(2-λ)-20 = λ^2 - 5λ -14 = 0\n",
        "  \\end{equation}\n",
        "   \\begin{equation}\n",
        " λ = \\frac{-b \\pm \\sqrt{b^2-4ac}}{2a} = \\frac{5 \\pm \\sqrt{81}}{2} = \\frac{5 \\pm 9}{2}= (7,-2)\n",
        "  \\end{equation}\n",
        "   \\begin{equation}\n",
        "   EigenVectors: (A-λI)\\vec{X}=\\vec{0} \\;,\\;λ_1=7 \\;,\\;λ_2=-2\n",
        "   \\end{equation}\n",
        " \\begin{equation}\n",
        "   λ_1=7 \\;,\\;\\begin{bmatrix}  3-7 & 5 \\\\ 4 & 2-7 \\end{bmatrix}\n",
        "     \\begin{bmatrix}  x_1 \\\\ x_2 \\end{bmatrix}=\\begin{bmatrix}  0 \\\\ 0 \\end{bmatrix} => \\begin{bmatrix}  -4x_1 + 5x_2 =0 \\\\ 4x_1-5x_2 =0\\end{bmatrix} =>\\end{equation}\n",
        "  \\begin{equation}\n",
        "     x_2=\\frac{4}{5}x_1 =>\\begin{bmatrix}  5 \\\\ 4\\end{bmatrix}\n",
        "\\end{equation}\n",
        " \\begin{equation}\n",
        "   λ_1=-2 \\;,\\;\\begin{bmatrix}  3+2 & 5 \\\\ 4 & 2+2 \\end{bmatrix}\n",
        "     \\begin{bmatrix}  x_1 \\\\ x_2 \\end{bmatrix}=\\begin{bmatrix}  0 \\\\ 0 \\end{bmatrix} => \\begin{bmatrix}  5x_1 + 5x_2 =0 \\\\ 4x_1+4x_2 =0\\end{bmatrix} =>\\end{equation}\n",
        "\\begin{equation}\n",
        "     x_2=-x_1=>\\begin{bmatrix}  -1 \\\\ 1\\end{bmatrix}\n",
        "\\end{equation}\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sRCavWTL6YN"
      },
      "source": [
        "### Question 2.4 (10p)\n",
        "Prove that a linear projection onto an $M$-dimensional subspace that maximizes the variance of the projected data is defined by the $M$ eigenvectors of the data covariance matrix $S$, corresponding to the $M$ largest eigenvalues. *Hint:* We showed this for the simple 1-D case in the TA session. Now extend this proof for the general case $M < D$ using induction.\n",
        "\n",
        "---\n",
        "\n",
        "**ANSWER HERE**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okJpynzHMpA6"
      },
      "source": [
        "## 3 Reinforcement Learning: Markov Decision Processes (32p)\n",
        "\n",
        "Suppose a robot is put in a maze with a long corridor. The\n",
        "corridor is 1 kilometer long and 5 meters wide. The available actions to the robot are moving forward 1 meter, moving backward 1 meter, turning left by 90 degrees and turning right by 90 degrees. If the robot moves and hits the wall, then it will stay in its position and orientation. The robot's goal is to escape from this maze by reaching the end of the long corridor.\n",
        "**Note: the answers in the following questions should not exceed 5 sentences.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rI3AATMANzC_"
      },
      "source": [
        "### Question 3.1 (4p)\n",
        "\n",
        "Assume the robot receives a +1 reward signal for each time step taken in the\n",
        "maze and +1000 for reaching the final goal (the end of the long corridor). Then you train the robot for a while, but it seems it still does not perform well at all for navigating to the end of the corridor in the maze. What is happening? Is there something wrong with the reward function?\n",
        "\n",
        "---\n",
        "\n",
        "**ANSWER HERE**\n",
        "\n",
        "The reward function is wrong, it is encouraging the robot to explore the maze taking more steps instead of going for the final goal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LC-aYmiFOAEZ"
      },
      "source": [
        "### Question 3.2 (4p)\n",
        "\n",
        "If there is something wrong with the reward function, how could you fix it? If not, how to resolve the training issues?\n",
        "\n",
        "---\n",
        "\n",
        "**ANSWER HERE**\n",
        "\n",
        "The reward function could be modified to not reward the robot at each time step, rewarding it only for reaching the goal, or even penalize with negative reward the time spent solving the maze, as to encourage it to reach the goal faster."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzIDv8qoOGuH"
      },
      "source": [
        "### Question 3.3 (2p)\n",
        "\n",
        "The discounted return for a non-episodic task is defined as\n",
        "$$\n",
        "G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\ldots\n",
        "$$\n",
        "where $\\gamma \\in [0, 1]$ is the discount factor.\n",
        "\n",
        "Rewrite the above equation such that $G_t$ is alone on the left hand side and $G_{t+1}$ is on the right hand side.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**ANSWER HERE**\n",
        "\n",
        "$$\n",
        "G_t = R_{t+1} + \\gamma G_{t+1}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTxEimD5O5eA"
      },
      "source": [
        "### Question 3.4 (2p)\n",
        "\n",
        "What is the sufficient condition for this infinite series to be a convergent series?\n",
        "\n",
        "---\n",
        "\n",
        "**ANSWER HERE**\n",
        "\n",
        "Bounded reward sequence and $γ<1 $\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bp30j2csPJkz"
      },
      "source": [
        "### Question 3.5 (5p)\n",
        "\n",
        "Suppose this infinite series is a convergent series, and each reward in the series is a constant of +1. We know the series is bounded, what is a simple formula for this bound ? Write it down without using summation.\n",
        "\n",
        "---\n",
        "\n",
        "**ANSWER HERE**\n",
        "\n",
        "It is a geometric series:\n",
        "$\\sum_{k=0}^{\\infty}{γ}^k$ with  $0\\leq\\gamma<1$.\n",
        "\n",
        "So it converges to $\\frac{1}{1-γ}$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmDQKWHNPSnx"
      },
      "source": [
        "### Question 3.6 (5p)\n",
        "\n",
        "Let the task be an episodic setting and the robot is running for $T = 5$ time steps. Suppose $\\gamma = 0.9$, and the robot receives rewards along the way $R_1 = −1, R_2 = −0.5, R_3 = 2.5, R_4 = 1, R_5 = 3$. What are the values for $G_0, G_1, G_2, G_3, G_4, G_5$?\n",
        "\n",
        "---\n",
        "\n",
        "**ANSWER HERE**\n",
        "\n",
        "\n",
        "  $G_0= R_1 + \\gamma G_1=-1+0.9 \\cdot 4.747 = 3.2723$\n",
        "\n",
        "  $G_1= R_2 + \\gamma G_2=-0.5+0.9 \\cdot 5.83 = 4.747$\n",
        "\n",
        "  $G_2= R_3 + \\gamma G_3=2.5+0.9\\cdot 3.7 = 5.83$\n",
        "\n",
        "  $G_3= R_4 + \\gamma G_4=1+0.9 \\cdot 3 = 3.7$\n",
        "\n",
        "  $G_4= R_5 + \\gamma G_5=3+0.9 \\cdot 0 = 3$\n",
        "\n",
        "  $G_5= R_6 + \\gamma G_6=0$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7ZcAEvfQA8f"
      },
      "source": [
        "### Question 3.7 (5p)\n",
        "\n",
        "Suppose each reward in the series is increased by a constant $c$, i.e. $R_t \\leftarrow R_t + c$.\n",
        "Then how does it change $G_t$?\n",
        "\n",
        "---\n",
        "\n",
        "**ANSWER HERE**\n",
        "$$\n",
        "G_t=\\sum_{k=0}^\\infty\\gamma^k(R_{t+1}+c)=\\sum_{k=0}^\\infty\\gamma^k(R_{t+1})+\\sum_{k=0}^\\infty\\gamma^kc=G_t + \\sum_{k=0}^\\infty\\gamma^kc\n",
        "$$\n",
        "so $G_t$ is increased by\n",
        "$\n",
        " \\sum_{k=0}^\\infty\\gamma^kc = \\frac{c}{1-\\gamma}\n",
        "$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7KPVdhPQBpz"
      },
      "source": [
        "### Question 3.8 (5p)\n",
        "\n",
        "Now consider episodic tasks, and similar to Question 3.6, we add a constant $c$ to each reward, how does it change $G_t$?\n",
        "\n",
        "---\n",
        "\n",
        "**ANSWER HERE**\n",
        "\n",
        "The additive term in question 3.6 is now :\n",
        "$\\sum_{i=0}^{T-t-1} \\gamma ^i c=\\sum_{i=0}^{T-t-1} \\gamma ^i c =c \\cdot \\dfrac{ 1 - \\gamma ^{T-t}}{1 -\\gamma} $\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjOnM7AVQVOB"
      },
      "source": [
        "## 4 Reinforcement Learning: Dynamic Programming (58p)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Jm0EyIHQaOp"
      },
      "source": [
        "### Question 4.1 (5p)\n",
        "\n",
        "Write down the Bellman optimality equation for the state value function without using expectation notation, but using probability distributions instead.\n",
        "Define all variables and probability distributions in bullet points.\n",
        "\n",
        "---\n",
        "\n",
        "**ANSWER HERE**\n",
        "$$V^*(s)=\\max_\\pi V_\\pi(s)=\\max_a\\sum_{s',r}p(s',r|s,a)[r+\\gamma V^*(s')]\\quad \\forall s$$\n",
        "\n",
        "where:\n",
        "\n",
        "*   $V^*(s')$ is the optimal state value function of the following state\n",
        "*   $s \\in S^{+}$ is a state and $s' \\in S^{+}$ is the following state\n",
        "*   $V_\\pi(s)$ is the state-value function for policy $\\pi$ and state $s$\n",
        "*   $\\pi \\in (a|s)$ is the policy\n",
        "*   $r \\in R^{+}$ is the reward\n",
        "*   $a \\in A^{+}$ is an action\n",
        "*$p(s',r|s,a)$ is the probability of each possible pair of next state, reward given any state, and action\n",
        "*   $\\gamma$ is the discount factor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0VZcK6LQkUC"
      },
      "source": [
        "### Question 4.2 (5p)\n",
        "\n",
        "Write down the Bellman optimality equation for the state-action value function without using expectation notation, but using probability distributions instead.\n",
        "Define all variables and probability distributions in bullet points.\n",
        "\n",
        "---\n",
        "\n",
        "**ANSWER HERE**\n",
        "\n",
        "$$Q^*(s,a)=\\max_\\pi Q_\\pi(s,a)=\\sum_{s',r}p(s',r|s,a)[r+\\gamma \\max_{a'}Q^*(s', a')]\\quad \\forall s$$\n",
        "\n",
        "\n",
        "where:\n",
        "*   $a \\in A^{+}$ is an action and $a' \\in A^{+}$ is the following action\n",
        "*   $s \\in S^{+}$ is a state and $s' \\in S^{+}$ is the following state\n",
        "*   $r \\in R^{+}$ is the reward\n",
        "*   $\\pi \\in (a|s)$ is the policy\n",
        "*   $p(s',r|s,a)$ is the probability of each possible pair of next state an reward given any state and action\n",
        "*   $\\gamma$ is the discount factor\n",
        "*   $Q_\\pi(s',a')$ is the action-value function for policy $\\pi$\n",
        "*   $Q^*(s',a')$ is the optimal action-value function for state $s'$ and action $a'$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tq66sRJeQlE2"
      },
      "source": [
        "### Question 4.3 (15p)\n",
        "\n",
        "Consider a 4x4 gridworld depicted in the following table:\n",
        "\n",
        "![Grid world](https://i.ibb.co/HdSdKJB/image.png)\n",
        "\n",
        "The non-terminal states are $S = \\{1, 2, \\ldots, 14\\}$ and the terminal states are $0, 15$.\n",
        "There are four available actions for each state, that is $A = \\{\\text{up}, \\text{down}, \\text{left}, \\text{right}\\}$.\n",
        "Assume the state transitions are deterministic and all transitions result in a negative reward of −1.\n",
        "If the agent hits the boundary, then its state will remain unchanged, e.g. $p(s=8, r=−1|s=8, a=\\text{left}) = 1$.\n",
        "Note: In this exercise, we assume the policy is a deterministic\n",
        "function and it initially maps each state to an arbitrary action.\n",
        "\n",
        "\n",
        "Manually run the policy iteration algorithm for one outer iteration. Use the in-place policy iteration algorithm (directly using the updated values for the next value update).\n",
        "To be specific, run the policy evaluation with a single pass through the states (16 equations, not until convergence) and one time policy improvement.\n",
        "Assume the initial state value for all 16 cells is 0.0 and the policy initially always outputs the 'left' action.\n",
        "Write down the equations and detailed numerical computations for the updated values of each cell.\n",
        "Use a discount factor $\\gamma = 0.5$.\n",
        "Write down the policy after policy improvement.\n",
        "\n",
        "Read more about this in Sutton & Barto's book http://www.incompleteideas.net/book/ebook/node43.html\n",
        "\n",
        "---\n",
        "\n",
        "**ANSWER HERE**\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "&V_\\pi(0) = 1 \\cdot [0 + 0.5 \\cdot 0] = 0 \\qquad & s' = 0  \\\\\n",
        "&V_\\pi(1) = 1 \\cdot [-1 + 0.5 \\cdot 0] = -1 \\qquad &  s' = 0  \\\\\n",
        "&V_\\pi(2) = 1 \\cdot [-1 + 0.5 \\cdot -1] = -1.5 \\qquad &  s' = 1  \\\\\n",
        "&V_\\pi(3) = 1 \\cdot [-1 + 0.5 \\cdot -1.5] = -1.75 \\qquad &  s' = 2  \\\\\n",
        "&V_\\pi(4) = 1 \\cdot [-1 + 0.5 \\cdot 0] = -1 \\qquad &  s' = 4  \\\\\n",
        "&V_\\pi(5) = 1 \\cdot [-1 + 0.5 \\cdot -1] = -1.5 \\qquad &  s' = 4  \\\\\n",
        "&V_\\pi(6) = 1 \\cdot [-1 + 0.5 \\cdot -1.5] = -1.75 \\qquad & s' = 5  \\\\\n",
        "&V_\\pi(7) = 1 \\cdot [-1 + 0.5 \\cdot -1.75] = -1.875 \\qquad &  s' = 6  \\\\\n",
        "&V_\\pi(8) = 1 \\cdot [-1 + 0.5 \\cdot 0] = -1 \\qquad &  s' = 8  \\\\\n",
        "&V_\\pi(9) = 1 \\cdot [-1 + 0.5 \\cdot -1] = -1.5 \\qquad &  s' = 8  \\\\\n",
        "&V_\\pi(10) = 1 \\cdot [-1 + 0.5 \\cdot -1.5] = -1.75 \\qquad &  s' = 9  \\\\\n",
        "&V_\\pi(11) = 1 \\cdot [-1 + 0.5 \\cdot -1.75] = -1.875 \\qquad &  s' = 10  \\\\\n",
        "&V_\\pi(12) = 1 \\cdot [-1 + 0.5 \\cdot 0] = -1 \\qquad &  s' = 12  \\\\\n",
        "&V_\\pi(13) = 1 \\cdot [-1 + 0.5 \\cdot -1] = -1.5 \\qquad &  s' = 12  \\\\\n",
        "&V_\\pi(14) = 1 \\cdot [-1 + 0.5 \\cdot -1.5] = -1.75 \\qquad &  s' = 13  \\\\\n",
        "&V_\\pi(15) = 1 \\cdot [0 + 0.5 \\cdot 0] = 0 \\qquad &  s' = 15\n",
        "\\end{aligned}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciourK1wQo-3"
      },
      "source": [
        "### Question 4.4 (12p)\n",
        "\n",
        "Implement the environment as described in the code skeleton below.\n",
        "Come up with your own solution and do not copy the code from a third party source.\n",
        "\n",
        "Then test your implementation of GridWorld using the implementation of policy iteration provided below. Run the code multiple times. Do you always end up with the same policy? Why? (max 4 sentences)\n",
        "\n",
        "---\n",
        "\n",
        "**ANSWER HERE**\n",
        "\n",
        "Yes. The algorithm converges to the global optimum of the value function, that is unique.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3gmuwlmdyO4"
      },
      "source": [
        "#### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkzicBh-I3dU"
      },
      "source": [
        "import numpy as np\n",
        "import itertools\n",
        "\n",
        "np.set_printoptions(precision=3, linewidth=180)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "side=4\n",
        "states = np.arange(4*4)\n",
        "print(states)\n",
        "states = np.arange(1,4*4-1)\n",
        "\n",
        "print(states)\n",
        "print()\n",
        "for i in range(0,side): print(i,end=\" \")\n",
        "print()\n",
        "for i in range(side*side-side,side*side): print(i,end=\" \")\n",
        "print()\n",
        "for i in range(side-1,side*side,side): print(i,end=\" \")\n",
        "print()\n",
        "for i in range(0,side*side,side): print(i, end=\" \")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHCP48whK5RY",
        "outputId": "f7e90778-87e1-417d-b820-bff2f05eb81e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15]\n",
            "[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
            "\n",
            "0 1 2 3 \n",
            "12 13 14 15 \n",
            "3 7 11 15 \n",
            "0 4 8 12 "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DT_bNpgqd2mM"
      },
      "source": [
        "\n",
        "#### Defining the problem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNBoBp3PJC0C"
      },
      "source": [
        "class GridWorld:\n",
        "\n",
        "    UP = 0\n",
        "    DOWN = 1\n",
        "    LEFT = 2\n",
        "    RIGHT = 3\n",
        "\n",
        "    def __init__(self, side=4):\n",
        "        self.side = side\n",
        "        # -------------------------\n",
        "        # Define integer states, actions, and final states as specified\n",
        "        # in the problem description\n",
        "\n",
        "        # TODO insert code here\n",
        "        self.states = np.arange(self.side * self.side)\n",
        "        self.actions = np.array([self.UP, self.DOWN, self.LEFT, self.RIGHT])\n",
        "        self.actions_str = [\"UP\", \"DOWN\", \"LEFT\", \"RIGHT\"]\n",
        "        self.finals = np.array([0, 15])\n",
        "\n",
        "        self.upSide=range(0,side)\n",
        "        self.downSide=range((side*side)-side,side*side)\n",
        "        self.rightSide=range(side-1,side*side,side)\n",
        "        self.leftSide= range(0,side*side,side)\n",
        "\n",
        "        # -------------------------\n",
        "        self.actions_repr = np.array(['↑', '↓', '←', '→'])\n",
        "\n",
        "    def _is_terminal(self, s):\n",
        "        # -------------------------\n",
        "        # Return True if s is a terminal state and False otherwise\n",
        "\n",
        "        # TODO insert code here\n",
        "        isTerminal=False\n",
        "        if(s in self.finals):\n",
        "          isTerminal=True\n",
        "        return isTerminal\n",
        "\n",
        "        # -------------------------\n",
        "\n",
        "    def _next_state(self, s, a):\n",
        "        # -------------------------\n",
        "        # Returns the next state of the environment if action a were taken\n",
        "        # while in state s\n",
        "\n",
        "        #print(\"STATE\")\n",
        "        #print(s)\n",
        "        #print(\"ACTION\")\n",
        "        #print(a)\n",
        "\n",
        "        # TODO insert code here\n",
        "        if(a==self.UP):\n",
        "            if(s in self.upSide): #first row\n",
        "              return s\n",
        "            return s-self.side\n",
        "\n",
        "        elif(a==self.DOWN):\n",
        "            if(s in self.downSide):\n",
        "                return s\n",
        "            return s+self.side\n",
        "\n",
        "        elif(a==self.LEFT):\n",
        "            if(s in self.leftSide):\n",
        "              return s\n",
        "            return s-1\n",
        "\n",
        "        elif(a==self.RIGHT):\n",
        "            if(s in self.rightSide):\n",
        "              return s\n",
        "            return s+1\n",
        "\n",
        "\n",
        "\n",
        "        #return s\n",
        "        # -------------------------\n",
        "\n",
        "    def _reward(self, s, s_next, a):\n",
        "        # -------------------------\n",
        "        # Return the reward for the given transition as specified\n",
        "        # in the problem description\n",
        "\n",
        "        # TODO insert code here\n",
        "        reward = -1\n",
        "        if(self._is_terminal(s)):\n",
        "          reward=0\n",
        "        return reward\n",
        "\n",
        "        # -------------------------\n",
        "\n",
        "    def reset(self):\n",
        "        # -------------------------\n",
        "        # Set the internal state of the environment to be sampled uniformly\n",
        "        # at random from the set of non-terminal states and return the state\n",
        "\n",
        "        # TODO insert code here\n",
        "        self.s = np.random.choice(np.arange(1,(self.side**2)-1))\n",
        "        return self.s\n",
        "\n",
        "        # -------------------------\n",
        "\n",
        "    def step(self, a):\n",
        "        # -------------------------\n",
        "        # Advances the environment one step using action a and returns s, r, T\n",
        "        # where s is the next state, r is the reward, and T is a boolean saying\n",
        "        # whether the episode is done or not\n",
        "\n",
        "        # TODO insert code here\n",
        "\n",
        "        #print(\"STATE\")\n",
        "        #print(self.s)\n",
        "        #print(\"ACTION\")\n",
        "        #print(a)\n",
        "\n",
        "        s_next=self._next_state(self.s,a)\n",
        "        r=self._reward(self.s,s_next,a)\n",
        "        T=self._is_terminal(s_next)\n",
        "\n",
        "        #print(\"NEXT\")\n",
        "        #print(self.s)\n",
        "        self.s=s_next\n",
        "        return self.s,r,T\n",
        "        # -------------------------\n",
        "\n",
        "    def print_policy(self, policy):\n",
        "        P = np.array(policy).reshape(self.side, self.side)\n",
        "        print(self.actions_repr[P])\n",
        "\n",
        "    def print_values(self, values):\n",
        "        V = np.array(values).reshape(self.side, self.side)\n",
        "        print(V)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrUMxh-qd5u0"
      },
      "source": [
        "#### Policy iteration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1DOXcH5J0NR"
      },
      "source": [
        "def transition_prob(s, s_next, a):\n",
        "    return 1 if problem._next_state(s, a) == s_next else 0\n",
        "\n",
        "\n",
        "def eval_policy(problem, policy, value, gamma=0.9, theta=0.01):\n",
        "    p = transition_prob\n",
        "    r = problem._reward\n",
        "\n",
        "    while True:\n",
        "        delta = 0\n",
        "        for s in problem.states:\n",
        "            v = value[s]\n",
        "            value[s] = np.sum(\n",
        "                [\n",
        "                    p(s, next_s, policy[s])\n",
        "                    * (r(s, next_s, policy[s]) + gamma * value[next_s])\n",
        "                    for next_s in problem.states\n",
        "                ]\n",
        "            )\n",
        "            delta = max(delta, abs(v - value[s]))\n",
        "\n",
        "        if delta < theta:\n",
        "            return value\n",
        "\n",
        "\n",
        "def improve_policy(problem, policy, value, gamma=0.9):\n",
        "    p = transition_prob\n",
        "    r = problem._reward\n",
        "\n",
        "    stable = True\n",
        "    for s in problem.states:\n",
        "        actions = problem.actions\n",
        "\n",
        "        b = policy[s]\n",
        "        policy[s] = actions[\n",
        "            np.argmax(\n",
        "                [\n",
        "                    np.sum(\n",
        "                        [\n",
        "                            p(s, next_s, a) * (r(s, next_s, a) + gamma * value[next_s])\n",
        "                            for next_s in problem.states\n",
        "                        ]\n",
        "                    )\n",
        "                    for a in actions\n",
        "                ]\n",
        "            )\n",
        "        ]\n",
        "        if b != policy[s]:\n",
        "            stable = False\n",
        "\n",
        "    return stable\n",
        "\n",
        "\n",
        "def policy_iteration(problem, gamma=0.9, theta=0.01):\n",
        "    # Initialize a random policy\n",
        "    policy = np.array([np.random.choice(problem.actions) for s in problem.states])\n",
        "    print(\"Initial policy\")\n",
        "    problem.print_policy(policy)\n",
        "    # Initialize values to zero\n",
        "    values = np.zeros_like(problem.states, dtype=np.float32)\n",
        "\n",
        "    # Run policy iteration\n",
        "    stable = False\n",
        "    for i in itertools.count():\n",
        "        print(f\"Iteration {i}\")\n",
        "        values = eval_policy(problem, policy, values, gamma, theta)\n",
        "        problem.print_values(values)\n",
        "        stable = improve_policy(problem, policy, values, gamma)\n",
        "        problem.print_policy(policy)\n",
        "        if stable:\n",
        "            break\n",
        "\n",
        "    return policy, values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqdoNw97mcEA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70c306c2-c80a-4c9e-95d0-14525463c3e6"
      },
      "source": [
        "# Run the below code, please include the output in your submission\n",
        "problem = GridWorld()\n",
        "policy_iteration(problem, gamma=0.5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial policy\n",
            "[['↑' '→' '↓' '←']\n",
            " ['→' '←' '←' '←']\n",
            " ['↑' '↑' '←' '↓']\n",
            " ['↑' '↑' '↑' '←']]\n",
            "Iteration 0\n",
            "[[ 0.    -1.999 -2.    -2.   ]\n",
            " [-1.999 -2.    -2.    -2.   ]\n",
            " [-2.    -2.    -2.    -1.5  ]\n",
            " [-2.    -2.    -2.    -1.   ]]\n",
            "[['↑' '←' '←' '←']\n",
            " ['↑' '↑' '↑' '↓']\n",
            " ['↑' '↑' '→' '↓']\n",
            " ['↑' '↑' '→' '↓']]\n",
            "Iteration 1\n",
            "[[ 0.    -1.    -1.5   -1.75 ]\n",
            " [-1.    -1.5   -1.75  -1.508]\n",
            " [-1.5   -1.75  -1.508 -1.008]\n",
            " [-1.75  -1.875 -1.008 -0.008]]\n",
            "[['↑' '←' '←' '←']\n",
            " ['↑' '↑' '↑' '↓']\n",
            " ['↑' '↑' '↓' '↓']\n",
            " ['↑' '→' '→' '↓']]\n",
            "Iteration 2\n",
            "[[ 0.    -1.    -1.5   -1.75 ]\n",
            " [-1.    -1.5   -1.75  -1.502]\n",
            " [-1.5   -1.75  -1.502 -1.002]\n",
            " [-1.75  -1.502 -1.002 -0.002]]\n",
            "[['↑' '←' '←' '←']\n",
            " ['↑' '↑' '↑' '↓']\n",
            " ['↑' '↑' '↓' '↓']\n",
            " ['↑' '→' '→' '↓']]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0, 2, 2, 2, 0, 0, 0, 1, 0, 0, 1, 1, 0, 3, 3, 1]),\n",
              " array([ 0.   , -1.   , -1.5  , -1.75 , -1.   , -1.5  , -1.75 , -1.502, -1.5  , -1.75 , -1.502, -1.002, -1.75 , -1.502, -1.002, -0.002], dtype=float32))"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hRFm6Zm6YI1"
      },
      "source": [
        "### Question 4.5 (5p)\n",
        "\n",
        "Let's run policy iteration with $\\gamma = 1$. Describe what is happening. Why is this the case? Give an example. What is $\\gamma$ trading off and how does it affect policy iteration? (max 8 sentences)\n",
        "\n",
        "---\n",
        "\n",
        "**ANSWER HERE**\n",
        "\n",
        "The convergence of the series is not guaranteed when $\\gamma = 1$. So if the series diverges, the program never terminates.\n",
        "\n",
        "When $\\gamma$ is closer to 1 the series takes longer to converge and the agent is more farsighted, takes more into account future rewards.\n",
        "\n",
        "When $\\gamma$ is closer to 0 the series converges sooner and the agent is more myopic.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rAQ1K_u6qtH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "outputId": "e671d1d9-b802-4178-b946-596b162d6e9f"
      },
      "source": [
        "policy_iteration(problem, gamma=1.0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial policy\n",
            "[['←' '↑' '↑' '↑']\n",
            " ['↑' '←' '↓' '→']\n",
            " ['↓' '→' '↑' '←']\n",
            " ['↓' '←' '←' '↑']]\n",
            "Iteration 0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-dc3c4e33788e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpolicy_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproblem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-28-fe2249eb40fc>\u001b[0m in \u001b[0;36mpolicy_iteration\u001b[0;34m(problem, gamma, theta)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Iteration {i}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproblem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0mproblem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mstable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimprove_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproblem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-fe2249eb40fc>\u001b[0m in \u001b[0;36meval_policy\u001b[0;34m(problem, policy, value, gamma, theta)\u001b[0m\n\u001b[1;32m     15\u001b[0m                     \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                     \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext_s\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0mnext_s\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mproblem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m                 ]\n\u001b[1;32m     19\u001b[0m             )\n",
            "\u001b[0;32m<ipython-input-28-fe2249eb40fc>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     13\u001b[0m             value[s] = np.sum(\n\u001b[1;32m     14\u001b[0m                 [\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                     \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext_s\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mnext_s\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mproblem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BiSlnccwuVDM"
      },
      "source": [
        "### Question 4.6 (16p)\n",
        "\n",
        "Implement Q-learning using the code skeleton below.\n",
        "Come up with your own solution and do not copy the code from a third party source.\n",
        "Then execute the block to show that your solution reached when $\\gamma = 0.5$ is optimal."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#tests\n",
        "print(np.random.random())\n",
        "print(np.random.randint(4))\n",
        "val = np.array([0.1,0.3,0.2,0.4])\n",
        "print(val)\n",
        "print(np.random.randint(4))\n",
        "print(np.argmax(val))\n",
        "print(np.zeros((len(problem.states), len(problem.actions)), dtype=float))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1XD8LeJaN7GF",
        "outputId": "d6a32c34-6994-4b1f-ea04-691759293d11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7466592998496482\n",
            "0\n",
            "[0.1 0.3 0.2 0.4]\n",
            "0\n",
            "3\n",
            "[[0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLBufZOFvTOJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69f7c6c4-dcab-45c0-ad6e-2ba2cfb47b24"
      },
      "source": [
        "problem = GridWorld()\n",
        "GAMMA = 0.5\n",
        "sa_values = np.zeros((len(problem.states), len(problem.actions)), dtype=float)\n",
        "\n",
        "def epsilon_greedy(a_values, epsilon):\n",
        "    # -------------------------\n",
        "    # This function takes a list a_values where each index i corresponds\n",
        "    # to an estimate of the value of action i and then performs\n",
        "    # epsilon-greedy action selection to sample and return an action\n",
        "\n",
        "    # TODO insert code here\n",
        "    if np.random.random() < epsilon:\n",
        "        return np.random.randint(len(a_values))\n",
        "    else:\n",
        "        return np.argmax(a_values)\n",
        "\n",
        "    # -------------------------\n",
        "\n",
        "iters = 100000\n",
        "#iters = 1000\n",
        "epsilon=0.5\n",
        "learning_rate=0.1\n",
        "\n",
        "for i in range(iters):\n",
        "    s = problem.reset()\n",
        "    #print(\"initial state\",s)\n",
        "    done = False\n",
        "    while not done:\n",
        "        # -------------------------\n",
        "        # Perform one step of Q-learning here using sa_values to store\n",
        "        # the action-value estimates and epsilon_greedy to perform the\n",
        "        # action selection\n",
        "        #\n",
        "        # Play around to find a good step size\n",
        "\n",
        "        # TODO insert code here\n",
        "\n",
        "\n",
        "        action = epsilon_greedy(sa_values[s], epsilon)\n",
        "        s_next, reward, done = problem.step(action)\n",
        "        sa_values[s, action] = sa_values[s, action] + learning_rate * \\\n",
        "          (reward + GAMMA* np.max(sa_values[s_next]) - sa_values[s, action])\n",
        "        s = s_next\n",
        "\n",
        "        #debug\n",
        "        #if(i%(iters/5)==0):\n",
        "          #print(i)\n",
        "          #print(\"action\",action)\n",
        "          #print(s,problem.actions_str[action],s_next, reward, done)\n",
        "          #print(sa_values)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # -------------------------\n",
        "\n",
        "optimal_policy_state_values = policy_iteration(problem, gamma=0.5)[1]\n",
        "learned_policy_state_values = np.max(sa_values, axis=1)\n",
        "\n",
        "print('Optimal Policy State Values:')\n",
        "print(optimal_policy_state_values)\n",
        "print('Learned Policy State Values:')\n",
        "print(learned_policy_state_values)\n",
        "print('Root Mean Squared Value Error: {0:.8f}'.format(\n",
        "    np.sqrt(np.mean(np.square(optimal_policy_state_values - learned_policy_state_values)))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial policy\n",
            "[['↑' '↓' '↑' '↑']\n",
            " ['→' '↑' '←' '↑']\n",
            " ['→' '↓' '↓' '→']\n",
            " ['↑' '↓' '←' '→']]\n",
            "Iteration 0\n",
            "[[ 0.    -2.    -1.992 -1.992]\n",
            " [-2.    -2.    -2.    -1.996]\n",
            " [-1.992 -1.992 -1.996 -1.992]\n",
            " [-1.996 -1.992 -1.996  0.   ]]\n",
            "[['↑' '←' '↑' '↑']\n",
            " ['↑' '↓' '↑' '↑']\n",
            " ['←' '↓' '←' '↓']\n",
            " ['↑' '↑' '→' '↓']]\n",
            "Iteration 1\n",
            "[[ 0.    -1.    -1.998 -1.998]\n",
            " [-1.    -1.998 -1.999 -1.999]\n",
            " [-1.998 -1.999 -2.    -1.   ]\n",
            " [-1.999 -2.    -1.     0.   ]]\n",
            "[['↑' '←' '←' '↑']\n",
            " ['↑' '↑' '↑' '↓']\n",
            " ['↑' '↑' '↓' '↓']\n",
            " ['↑' '→' '→' '↓']]\n",
            "Iteration 2\n",
            "[[ 0.   -1.   -1.5  -2.  ]\n",
            " [-1.   -1.5  -1.75 -1.5 ]\n",
            " [-1.5  -1.75 -1.5  -1.  ]\n",
            " [-1.75 -1.5  -1.    0.  ]]\n",
            "[['↑' '←' '←' '↓']\n",
            " ['↑' '↑' '↑' '↓']\n",
            " ['↑' '↑' '↓' '↓']\n",
            " ['↑' '→' '→' '↓']]\n",
            "Iteration 3\n",
            "[[ 0.   -1.   -1.5  -1.75]\n",
            " [-1.   -1.5  -1.75 -1.5 ]\n",
            " [-1.5  -1.75 -1.5  -1.  ]\n",
            " [-1.75 -1.5  -1.    0.  ]]\n",
            "[['↑' '←' '←' '↓']\n",
            " ['↑' '↑' '↑' '↓']\n",
            " ['↑' '↑' '↓' '↓']\n",
            " ['↑' '→' '→' '↓']]\n",
            "Optimal Policy State Values:\n",
            "[ 0.   -1.   -1.5  -1.75 -1.   -1.5  -1.75 -1.5  -1.5  -1.75 -1.5  -1.   -1.75 -1.5  -1.    0.  ]\n",
            "Learned Policy State Values:\n",
            "[ 0.   -1.   -1.5  -1.75 -1.   -1.5  -1.75 -1.5  -1.5  -1.75 -1.5  -1.   -1.75 -1.5  -1.    0.  ]\n",
            "Root Mean Squared Value Error: 0.00000000\n"
          ]
        }
      ]
    }
  ]
}